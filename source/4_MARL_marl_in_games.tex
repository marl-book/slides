% !TEX program = xelatex
%%%%%%%%%%%%%%%%%%%%%%%%

\input{preamble.tex}
\leoslide

\subtitle{Multi-Agent Reinforcement Learning in Games: First Steps and Challenges}

\begin{document}
\maketitle

\introslide

\begin{frame}{\outline}

\blist
    \item Learning framework for MARL
    \item Independent learning
    \item Central learning
    \item Modes of learning
    \item Challenges of MARL
\elist
\end{frame}

\section{The MARL learning framework}

\begin{frame}[t]{MARL Learning Process}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth, height = 0.8\textheight, keepaspectratio]{images/chapter_5/learning-process.pdf}
    \label{fig:enter-label}
\end{figure}

\blist
    \item The game model defines agent interaction
    \item Data of these interactions are collected as $\mathcal{D}^{z} = \{ h^{t_e} \mid e = 1, \ldots, z\}, z \geq 0$
    \item A learning algorithm updates each agent's policy $\pi^{z+1} = \mathbb{L}(\mathcal{D}^{z}, \pi^{z})$
    \item The learning goal is a chosen solution concept, e.g. Nash equilibrium
\elist
    
\end{frame}

% \begin{frame}{MARL Learning Process - Continued}

% There are several additional nuances to the MARL learning process.

% \blist
%     \item The chosen game model will determine what the learnt joint policy conditions on:
%     \vspace{-15pt}
%     \begin{itemize}
%         \item Normal-form games: simple probability distribution across actions
%         \item Repeated normal-form games: condition on action histories $h^t = (a^0, ..., a^{tâ€“1})$
%         \item Stochastic games: condition on state-action histories $h^t = (s^0, a^0, s^1, a^1, ..., s^t)$
%         \item POSG: condition on independent observation histories $h^t_i = (o_i^0, ..., o^t_i)$
%     \end{itemize}
%     \item Policies can however be adjusted as required e.g. in a stochastic game only condition on the last state-action 
%     \item The learning algorithm $\mathbb{L}$ can consists of multiple learning algorithms e.g. $\mathbb{L}_i \forall i \in I$
%     \item Each $\mathbb{L}_i$ may use different parts of the data, or use independently collected data
% \elist

% \end{frame}

\begin{frame}[t]{Game Model's and Joint Policy}

% The chosen game model will determine what the learnt joint policy conditions on:

\begin{figure}
    \centering
    \begin{minipage}[t]{0.39\linewidth}
        % First Column
        \centering
        \vspace{0.5cm}
        \visible<1->{
            \includegraphics[width=0.8\linewidth, keepaspectratio]{images/chapter_4/nf_inputs-1.pdf}
            \textbf{Normal-form games}
        }
        \vspace{1.5cm}
        \visible<2->{
            \includegraphics[width=\linewidth]{images/chapter_4/rnf_input-1.pdf} 
            \textbf{Repeated normal-form \\ games}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.59\linewidth}
        \centering
        \visible<3->{
            \includegraphics[width=0.75\linewidth]{images/chapter_4/sg_input-1.pdf}\\
            \textbf{Stochastic games}
        }
        \vspace{0.5cm}
        \visible<4->{
            \includegraphics[width=0.75\linewidth]{images/chapter_4/posg_input-1.pdf}\\
            \textbf{Partially observable stochastic games}
        }
    \end{minipage}
\end{figure}

\end{frame}

\begin{frame}{Convergence}

To evaluate the learning algorithm, we typically assess whether the learnt joint policy has \fat{converged} to the optimal joint policy. 
\vspace{0pt}
\[
\lim_{z \to \infty}\pi^z = \pi^*
\]
\vspace{-15pt}
\blist
    \item The optimal joint policy may differ depending on the chosen game solution
    \item There may be many valid solutions, e.g., multiple Nash equilibria
    \item In practice, we cannot collect infinite data
    \item Learning typically stops after a predefined 'budget' (e.g. training steps) is reached or changes in the policy are below a predefined threshold
    % \item Alternative theoretical evaluation criteria include:
    % \begin{itemize}
    %     \item Convergence of average return
    %     \item Convergence of empirical distribution of the learn policy to the optimal policy
    % \end{itemize}
\elist
    
\end{frame}


\begin{frame}[t]{Single-Agent RL Reductions}

The simplest way to apply reinforcement learning algorithms in multi-agent settings is to reduce them to single-agent problems. 

\blist
    \item We can apply single-agent RL algorithms to each agent independently
    \begin{itemize}
        \item This is known as \fat{independent} learning; agents do not explicitly consider each other instead they treat other agents as part of the environment
    \end{itemize}
    \item Alternatively, we can apply one single-agent RL to all agents centrally 
    \begin{itemize}
        \item This is referred to as \fat{central} learning, where a central policy is learned, providing action probability across all agents' actions
    \end{itemize}
\elist
    
\end{frame}

\section{Central Learning}

\begin{frame}{Central Learning}

In the central learning framework, we learn a single central policy $\pi_c$, which receives observations of all agents and selects an action for each agent. 

\blist
    \item Requires transforming the joint reward $(r_1, \ldots, r_n)$, into a single scalar reward $r$
    \item This can be easy in some settings, e.g. in games with common rewards $r = r_i$, but difficult in zero-sum or general-sum games
    \item Central learning does not scale well with the number of agents as the joint action space grows exponentially with the number of agents
    \item This framework may also not be suitable in environments that require agents to act independently based on localized observations
\elist
\end{frame}

\begin{frame}{Central Q-Learning}


\balg{Central Q-learning}{cql}
\State Initialize: \( Q(s, a) = 0 \) for all \( s \in S \) and \( a \in A = A_1 \times \ldots \times A_n \)
\State Repeat for every episode:
\For{\( t = 0, 1, 2, \ldots \)}
    \State Observe current state \( s^t \)
    \State With probability \( \epsilon \): choose random joint action \( a^t \in A \)
    \State Otherwise: choose joint action \( a^t \in \arg\max_a Q(s^t, a) \)
    \State Apply joint action \( a^t \), observe rewards \( r_1^t, \ldots, r_n^t \) and next state \( s^{t+1} \)
    \State Transform \( r_1^t, \ldots, r_n^t \) into scalar reward \( r^t \)
    \State \( Q(s^t, a^t) \leftarrow Q(s^t, a^t) + \alpha [ r^t + \gamma \max_{a'} Q(s^{t+1}, a') - Q(s^t, a^t) ] \)
\EndFor
\ealg

\end{frame}

\section{Independent Learning}

\begin{frame}{Independent Learning}

In the independent learning framework, each agent $i$ learns its policy $\pi_i$ using only its local history of observations, treating the effects of other agents' actions as part of the environment.

\blist
    \item From the perspective of the individual agent, the environment transition function looks like this:
\elist
\vspace{5pt}
\[
\mathcal{T}_i(s^{t+1} | s^t, a_i) \propto \sum_{a_{-i} \in A_{-i}} \mathcal{T}(s^{t+1} | s^t, \langle a_i, a_{-i} \rangle) \prod_{j \neq i} \pi_j(a_j | s^t)
\]
% \vspace{5pt}
\blist
    \item As each agent $j$'s policies are updated, the action probabilities $\pi_j$ change
    \item From agent $i$'s perspective its transition function $\mathcal{T}_i$ thus appears to be non-stationary
\elist
    
\end{frame}

\begin{frame}{Independent Q-learning}

\balg{Independent Q-learning (IQL) for stochastic games}{iql}
\State // \textit{Algorithm controls agent} \( i \)
\State Initialize: \( Q_i(s, a_i) = 0 \) for all \( s \in S \), \( a_i \in A_i \)
\State Repeat for every episode:
\For{\( t = 0, 1, 2, \ldots \)}
    \State Observe current state \( s^t \)
    \State With probability \( \epsilon \): choose random action \( a_i^t \in A_i \)
    \State Otherwise: choose action \( a_i^t \in \arg\max_{a_i} Q_i(s^t, a_i) \)
    \State (meanwhile, other agents \( j \neq i \) choose their actions \( a_j^t \))
    \State Observe own reward \( r_i^t \) and next state \( s^{t+1} \)
    \State \( Q_i(s^t, a_i^t) \leftarrow Q_i(s^t, a_i^t) + \alpha [ r_i^t + \gamma \max_{a'_i} Q_i(s^{t+1}, a'_i) - Q_i(s^t, a_i^t) ] \)
\EndFor
\ealg
\end{frame}

\begin{frame}{IQL and CQL in Level-based Foraging}

\begin{columns}
    \begin{column}{0.5\textwidth}
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth,height=.8\textheight,keepaspectratio]{images/environments/lbf/tabular_marl_lbf_annot.pdf}
        \label{fig:enter-label}
        
    \end{figure}
        
    \end{column}
    
    \begin{column}{0.5\textwidth}
    \vspace{10pt}
        \centering
        \includegraphics[width=\textwidth,height=.8\textheight,keepaspectratio]{images/chapter_5/tabular_marl_lbf_returns_iql_cql.pdf}
        \label{fig:enter-label}
        \blist
            \item IQL can learn more quickly as CQL needs to explore $6^2 = 36$ actions in each state
        \elist
        
    \end{column}
\end{columns}
    
\end{frame}

\section{Modes of Operation in MARL}

\begin{frame}{Modes of Learning}

In addition to independent and central learning, there are different modes of learning algorithms. Algorithms can use \fat{self-play} or \fat{mixed-play}.

\fat{Self-play}:

\blist
    \item Refers to two related but distinct modes of operation in MARL, \fat{algorithm self-play} and \fat{policy self-play}
    \item \fat{Algorithm self-play} assumes that all agents use the same learning algorithm
    \item \fat{Policy self-play} is more literal and means that an agent's policy is trained directly against itself
\elist

\fat{Mixed-play}

\blist
    \item Refers to instances where agents use different learning algorithms
    % \item Trading bots in markets that use RL are one example of a mixed-play scenario
    % \item Ad-hoc teamwork is another area of research that focuses on this setting where agents must collaborate with previously unknown agents
\elist
    
\end{frame}

\section{MARL Challenges}

\begin{frame}[t]{MARL Challenges}

MARL algorithms inherit issues from single-agent RL, such as:

\blist
    \item Unknown environment dynamics
    \item Exploration-exploitation dilemma
    \item Non-stationarity from bootstrapping
    \item Temporal credit assignment
\elist

In addition to this, MARL algorithms face challenges that arise from learning in a dynamic multi-agent system.
    
\end{frame}

\begin{frame}{Non-Stationarity}

\underline{A stochastic process ${X^t}_{t \in \mathbb{N}^0}$ is stationary if:}

\begin{itemize}
    \item The probability distribution of $X^{t + \tau}$ does not depend on $\tau \in \mathbb{N}^0$, where t and $t + \tau$ are time indices
    \item This means that the dynamics of the process do not change over time
\end{itemize}

\underline{Now $X^t$ samples the state $s^t$ at each time step $t$:}

\begin{itemize}
    \item In a MDP, $X^t$ is completely defined by the state transition function $\mathcal{T}(s^t|s^{t-1}, a^{t-1})$ and the agents policy $\pi$ which selects an action $a \sim \pi(.|s)$
    \item The \textbf{Markov property} means that this property is stationary in that $s^t$ depends only on $s^{t-1}$, $a^{t-1}$ and $a^{t-1}$ depends only on  $s^{t-1}$ via $\pi(.|s^{t-1})$
    \item In RL, the policy does, however, change over time through the learning process $\pi^{z+1} = \mathbb{L}(\mathcal{D}^z, \pi^z)$,  which leads to \fat{non-stationarity} in $X^t$
\end{itemize}

\end{frame}

\begin{frame}{Non-Stationarity in Multi-Agent Settings}

In MARL, non-stationarity is exacerbated by multiple agents changing their policies over time.
\vspace{0pt}
\begin{columns}
\begin{column}{0.5\textwidth}
    % \vspace{-5pt}
    \begin{figure}
        \includegraphics[width=0.8\textwidth, height = 0.8\textheight, keepaspectratio]{images/chapter_5/wolfphc_rps_own.pdf}

    \end{figure}
\end{column}

\begin{column}{0.5\textwidth}
\blist
    \item $\pi^{z+1} = \mathbb{L}(\mathcal{D}^z, \pi^z)$ updates an entire joint policy $\pi^z = (\pi_1^z, ..., \pi_n^z)$
    \item Leads the entire environment to seem non-stationarity from each agent's perspective
    \item Can cause cyclic learning dynamics where agents co-adapt to each other's changing policies 
    % \item Stochastic approximation conditions that ensure convergence in single-agent RL don't apply in MARL
\elist
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Equilibrium Selection}

A game might have multiple equilibrium solutions, each of which might yield different expected returns to the agents in the game. This leads to the problem of \fat{equilibrium selection}
\vspace{5pt}
\begin{columns}
    \begin{column}{0.5\textwidth}
        \blist
            \item Consider the stag hunt game
            \item Two hunters choose: cooperate to hunt stag (S) or go solo for hare (H)
            \item Nash equilibria: reward-dominant (S,S) maximizes reward, risk-dominant (H,H) minimizes risk
            \item (S,S) requires trust; (H,H) offers a safe, lower reward
        \elist
    \end{column}
    \begin{column}{0.5\textwidth}
        \centering
        \gamestaghunt
    \vspace{10pt}
        \blist
            \item Independent Q-learning may bias towards (H,H) due to initial action uncertainty
            \item Early random actions can reinforce (H,H) since deviating from H is penalized if the other agent chooses H
        \elist
    \end{column}
\end{columns}
    
\end{frame}

\begin{frame}{Multi-Agent Credit Assignment}

Credit assignment in single-agent RL refers to determining which past action contributes to receiving rewards. In MARL, determining which agent has contributed to receiving rewards is an additional challenge.

\begin{columns}
    \begin{column}{0.5\textwidth}
        \begin{figure}
            \centering
            \includegraphics[width=0.8\textwidth, height = 0.8\textheight, keepaspectratio]{images/environments/lbf/foraging_8x12_b.png}
        \end{figure}
    \end{column}
    \begin{column}{0.5\textwidth}
        \blist
            \item At time step $t$ all agents simultaneously perform \textit{collect}, each receiving reward $+1$
            \item Whose action led to the reward?
            \item The agent on the left did not contribute
            \item For a learning agent which only observes $s^t, a^t, r^t, s^{t+1}$, disentangling the action contributions is difficult
        \elist
    \end{column}
\end{columns}
    
\end{frame}
\begin{frame}{Joint Actions for Addressing Multi-Agent Credit Assignment}

\fat{Joint actions} can help disentangle agent contributions. Consider the RPS game:

\centering
\gamerps 

\begin{enumerate}
    \item Agents choose actions $(a_1, a_2) = (R, S)$ with agent $1$ receiving reward $+1$
    \item Agents choose $(a_1, a_2) = (R, P)$ with agent $1$ receiving reward $-1$
    \item Using $Q(s, a_1)$ does not model agent $2$'s action's effects and thus the value for action $R$ appears to be 0
    \item $Q_1(s, a_1, a_2)$ can represent the effect of agent $2$ thus ascribing different values to joint action $(R, S)$ and $(R, P)$
\end{enumerate}

\end{frame}

\begin{frame}{Scaling to Many Agents}

The ability to scale to many agents is an important goal in MARL. Scaling agents is, however, a significant challenge for MARL as:

\begin{columns}
    \begin{column}{0.4\textwidth}
        \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth, height = 0.8\textheight, keepaspectratio]{images/environments/lbf/foraging_8x12_b.png}
        \end{figure}
    \end{column}
    \begin{column}{0.6\textwidth}
    \blist
        \item \# of \textbf{joint action} can grow \textbf{exponentially} with \# agents since $|A| = |A_1| \cdot ... \cdot |A_n|$
        \item Changing \# agents from 3 to 5 increases the number of joint actions from 216 to 7776
        \item If agents have associated features in $s$ (e.g. agent position) then $|S|$ also increases exponentially
        \item In CL, this increases the decision problem, while in IL this increases issues of non-stationarity and multi-agent credit assignment
    \elist
    \end{column}
\end{columns}
% \blist

%     \item In level-based foraging from slide 16, if we change the number of agents from 3 to 5, we also increases the number of joint actions from 216 to 7776
%     \item if the agents have their associated features in the state s, as they do in level-based foraging (agent positions), then the number of states $|S|$ also increases exponentially
%     \item Central learning algorithms face challenges in the increased size of the decision problem, while independent learning suffers from increased issues of non-stationarity and multi-agent credit assignment
% \elist
% \end{columns}
\end{frame}

\begin{frame}{Summary}

\fat{We covered:}
    \blist
        \item MARL Learning Process
        \item Independent and central learning
        \item Modes of learning in MARL
        \item Challenges of MARL
    \elist

\fat{Next we'll cover:}

    \blist
        \item Foundational algorithms in MARL
    \elist
    
\end{frame}

\end{document}

