% !TEX program = xelatex
%%%%%%%%%%%%%%%%%%%%%%%%

\input{preamble.tex}
\leoslide

\subtitle{Introduction}

\begin{document}
\maketitle

\introslide

\begin{frame}{\outline}
    \vspace{10pt}
    \fat{Part 1: Multi-Agent Reinforcement Introduction} 
    \vspace{5pt}
    \blist
        \item Multi-agent systems
        \item Advantages of MARL vs SARL in multi-agent systems.
        \item Challenges of MARL   
    \elist
    \vspace{10pt}
    \fat{Part 2: Reinforcement Learning Basics} 
    \vspace{5pt}
    \blist
        \item Markov-decision process.
        \item Discounted returns.
        \item Dynamic programming and temporal-difference learning.
    \elist
\end{frame}

\section{Part 1: Multi-Agent Reinforcement Introduction}

\begin{frame}{What is MARL?}
		\textbf{\textit{Multi-agent reinforcement learning (MARL) is about finding optimal decision policies for two or more artificial agents interacting in a common environment.}}
  \vspace{20pt}
  \blist
    \item Applying reinforcement learning (RL) algorithms to multi-agent systems. 
    \item The aim is to learn optimal policies for two or more agents independently. 
  \elist
\end{frame}

\begin{frame}{MARL Applications}
  \begin{figure}
    \centering
    % Top left image
    \begin{minipage}{.5\linewidth}
        \centering
        \includegraphics[width=.65\linewidth, keepaspectratio]{images/1_starcraft.png}
        % Optional short caption for the first image
        \caption{Competitive games}
    \end{minipage}%
    % Top right image
    \begin{minipage}{.5\linewidth}
        \centering
        \includegraphics[width=.7\linewidth, keepaspectratio]{images/1_self_driving.png}
        % Optional short caption for the second image
        \caption{Autonomous driving}
    \end{minipage}
    
    % Bottom left image
    \begin{minipage}{.5\linewidth}
        \centering
        \includegraphics[width=.65\linewidth, keepaspectratio]{images/1_reware.png}
        % Optional short caption for the third image
        \caption{Multi-robot warehouse management}
    \end{minipage}%
    % Bottom right image
    \begin{minipage}{.5\linewidth}
        \centering
        \includegraphics[width=.65\linewidth, keepaspectratio]{images/1_trading.png}
        % Optional short caption for the fourth image
        \caption{Automated trading in electronic markets}
    \end{minipage}
  \end{figure}
\end{frame}

\begin{frame}{Mutli-Agent Systems}

    Multi-agent systems consist of:
    \blist
        \item \textbf{An environment:} The environment is a physical or virtual world whose state evolves and is influenced by the agents' actions within the environment. 
        \item \textbf{Agents:} An agent is an entity which receives information about the state of the environment and can choose actions to influence the state. Agents are goal-directed, e.g. maximizing returns. 
    \elist
\end{frame}

% Need to edit potentially using columns. 
\begin{frame}{Multi-Agent Systems}		
    \begin{figure}
        \centering
        \includegraphics[width=\textwidth,height=.9\textheight,keepaspectratio]{images/chapter_1/mas_schematic.pdf}
        \label{fig:enter-label}
    \end{figure}
\end{frame}

\begin{frame}{Level Based Foraging Example}

    \begin{figure}
        \centering
        \includegraphics[width=\textwidth,height=.425\textheight,keepaspectratio]{images/environments/lbf/foraging_8x12_b.png}
        \label{fig:enter-label}
    \end{figure}

    \blist
        \item 3 agents with varying skill levels.
        \item Goal: to collect all apples.
        \item Items can be collected if a group of one or more agents are located next to the item and the sum of all agents levels is greater or equal to the item level. 
        \item Action space $A$ is $\{up, down, left, right, collect, noop\}$
    \elist
\end{frame}

\begin{frame}{MARL for solving Multi-Agent Systems}

    \begin{figure}
        \centering
        \includegraphics[width=.6\textwidth]{images/chapter_1/MARL-loop.pdf}
    \end{figure} 

    \vspace{-.5em}

    \blist
        \item Goal: learn optimal policies for a set of agents in a multi-agent system.
        \item Each agent chooses an action based on its policy; the actions of all agents together form a joint action. 
        \item The environment transitions to a new state based on the joint action.
    \elist
\end{frame}

\begin{frame}{Why MARL?}
\small

 Why should we use MARL to find optimal solutions to multi-agent systems rather than controlling multiple 'agents' using a single-agent RL (SARL) algorithm? I.e. one agent controlling the actions of all agents. 

\vspace{10pt}
  \begin{columns}[T]
    \begin{column}{0.45\textwidth}
        \e{Decomposing a larger problem}
        \blist
            \item For example, controlling $3$ agents each with $6$ actions (see LBF example), the action space becomes $6^3 = 216$.
            \item Using MARL, we decompose this into three more tractable problems. 
        \elist
    \end{column}
    \hspace{10pt}
    \begin{column}{0.45\textwidth}
        \e{Decentralized decision making}
        \blist
            \item There are many real-world scenarios where it is beneficial for each agent to make decisions independently.
            \item E.g. for autonomous driving is impractical for frequent long-distance data exchanges between a central agent and the vehicle.
        \elist
    \end{column}
\end{columns}
\end{frame}

\begin{frame}{Challenges of MARL}

    While it might be advantageous to use MARL in certain settings, several challenges arise or are amplified in \fat{MARL} compared to \fat{SARL}.
    \vspace{10pt}
    \blist
        \item Non-stationarity caused by learning agents
        \item Optimality of policies and equilibrium selection
        \item Multi-agent credit assignment
        \item Scaling in number of agents
    \elist
    \vspace{10pt}
    We will explore these challenges more thoroughly in upcoming lectures. 
    
\end{frame}

% \begin{frame}{MARL Applications of }

%     \fat{Competitive Gaming:}
%     \blist
%         \item Video and Board Games (Backgammon, Chess, Go, Starcraft etc.).
%     \elist
%     \fat{Warehousing:}
%     \blist
%         \item Warehouse Robot Management, robots that can 
%         retrieve items requested by users from the warehouse).
%     \elist
%     \fat{Autonomous Driving:}
%     \blist
%         \item Autonomous Driving (controlling policies for multiple vehicles to navigate through complex interaction. 
%     \elist
%     \fat{Trading Bots:}
%     \blist
%         \item Automated Trading in Electronic Markets.
%     \elist
    
% \end{frame}

\section{Part 2: Reinforcement Learning Basics}


\begin{frame}{Back to RL Basics}

    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth,height=0.4\textheight,keepaspectratio]{images/chapter_2/rl-learning-problem.pdf}
        \label{fig:enter-label}
    \end{figure}

    \blist
        \item RL algorithms learn solutions for sequential decision problems via repeated environment \textbf{interaction}.
        \item The \textbf{goal} is to learn an optimal decision policy for a specific objective within an environment.  
        \item A sequential decision process is usually defined more formally as a Markov decision process (MDP).
    \elist
    
\end{frame}

\begin{frame}{MDP Interaction}
    
    \begin{figure}[htbp]
      \centering
      \includegraphics[width=0.9\textwidth,height=0.9\textheight,keepaspectratio]{images/1_mdp_diagram_elipses.pdf}
      \label{fig:mdp}
    \end{figure}
    
\end{frame}

\begin{frame}{MDP continued}

  \begin{columns} % The [T] option aligns column content to the top

    % Left column for text
    \begin{column}{0.5\textwidth}
     A MDP can be defined as a 5 tuple $(S, A, \mathcal{R}, \mathcal{T}, \mu):$
      \blist
        \item \textcolor{blue}{$S$}: Finite set of states with subset of terminal states $\bar{S} \subset S$
        \item \textcolor{red}{$A$}: Finite set of actions. 
        \item \textcolor{green}{$\mathcal{R}$}: Reward function $\mathcal{R}: \textcolor{blue}{S} \times \textcolor{red}{A} \times \textcolor{blue}{S} \to \mathbb{R}$
        \item $\mathcal{T}:$ State transition function $\mathcal{T}: \textcolor{blue}{S} \times \textcolor{red}{A} \times \textcolor{blue}{S} \to [0, 1]$
        \item $\mu$: Initial state distribution $\mu: S \to [0,1]$ such that $\sum_{s\in S}\mu (s) = 1$ and $\forall s \in \hat{S}: \mu (s) = 0$
      \elist
    \end{column}

    % Right column for images
    \begin{column}{0.5\textwidth}
      \begin{figure}
        \includegraphics[width=\linewidth]{images/1_mdp_diagram.pdf}
      \end{figure}
    \end{column}

  \end{columns}

\end{frame}

\begin{frame}{MDP assumptions}

    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \fat{\e{Markov Property}}
            \blist
                \item Future states are temporally independent of past states and actions, given the current state and action. 
                \item $Pr(s^{t+1}, r^{t}|s^{t}, a^{t}, s^{t-1}, a^{t-1}, ...,s^0, a^0) = Pr(s^{t+1}, r^{t}|s^{t}, a^{t})$
                % \item Meaning that the current state provides all necessary information for an agent to choose the optimal action. 
            \elist 
            \vspace{10pt}
            \fat{\e{Full Observability}}
            \blist
                \item The agent knows the entire state of the world. 
                \item In practice, this is often violated, and we have Partially Observable MDPs (POMDP).
            \elist       
        \end{column}
        
        \hspace{10pt}
        
        \begin{column}{0.5\textwidth}
            
            \fat{\e{Stationarity}}
            \blist
                \item The dynamics of the environment are assumed to be stationary. 
                \item i.e. $\mathcal{T}$ and \textcolor{green}{$\mathcal{R}$} remain constant through time. 
            \elist
            \vspace{10pt}
            \fat{\e{No Knowledge of MDP Dynamics}}
            \blist
                \item In RL it is assumed that the agent has knowledge only of the action and state spaces ($\textcolor{red}{A}, \textcolor{blue}{S}$)
                \item The transition and reward function ($\mathcal{T}, \mathcal{R}$) are assumed to be unknown.
            \elist
        
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Mars Rover MDP Example}
    \begin{columns}[onlytextwidth] 
    
      \begin{column}{0.5\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth,height=.8\textheight,keepaspectratio]{images/chapter_2/mdp-rover.pdf}
          \label{fig:enter-label}
        \end{figure}
      \end{column}
    
      % Column for the text
      \begin{column}{0.5\textwidth}
        \blist
          \item \textit{Start} is the initial state \(s^0\)
          \item Two possible actions \(A = \{right, left\}\)
          \item Goal is to get to \textit{Base}
          \item Rewards given by $\mathcal{R}(s, a, s')$ are shown as $r$. 
          \item State transition probabilities given by $\mathcal{T}(s, a, s)$, are shown as $p$
        \elist
      \end{column}
    
    \end{columns}
\end{frame}

\begin{frame}{RL for optimizing decision-making in MDPs}

    \begin{figure}
        \centering
        \includegraphics[width=\textwidth,height=.5\textheight,keepaspectratio]{images/chapter_2/RL-loop.pdf}
        \label{fig:rl_loop}
    \end{figure}
    \vspace{20pt}
    \begin{columns}
    
        \begin{column}{0.5\textwidth}
            \e{\fat{Value-Based RL}} \\
            These methods indirectly update the policy by approximating value functions.
        \end{column}

        \begin{column}{0.4\textwidth}
            \e{\fat{Policy-Based RL}} \\
            These methods update a parameterized policy function directly.
                % \item These methods also include hybrid approaches (actor-critic) that update a parameterised policy function but use value function approximation as a baseline. 
      \end{column}
    
    \end{columns}
    
\end{frame}

\begin{frame}{Expected Discounted Returns}

    Using RL, we aim to maximize the expected sum of \textbf{returns}.

    \blist
        \item Returns ($u$) are the sum of rewards received through time
        \item Typically, we \textbf{discount} returns as this ensures finite (discounted) returns (if discount factor $\gamma < 1$)
    \elist
    \vspace{0pt}
    $$
    \mathbb{E}_{\pi}[u_{t}] = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^t r^t\right]
    $$

    \blist
        \item $\gamma$ is the discount factor, such that $\gamma \in [0, 1]$
        \item $\pi$ is the behavior policy, that determines which actions are chosen. (We discuss these further in slide 16)
    \elist
\end{frame}

\begin{frame}{State-Value Functions}

    \textbf{State-value} functions $V^{\pi}(s)$, give the 'value' of state $s$, when following the policy $\pi$. 
    \vspace{0pt}
    \begin{align*}
    V^{\pi}(s) = \mathbb{E}_\pi \left[u^{t} | s^{t} = s \right]
    \end{align*}
    \vspace{0pt}
    The return ($u$) can be recursively defined as:
    \vspace{0pt}
    \begin{align*}
            u^t &= r^t +\gamma (r^{t+1} + \gamma r^{t+2} + ... )  \\ &= r^t + \gamma u^{t+1}
    \end{align*}
    \vspace{0pt}   
    Therefore:
    \vspace{0pt}
    \begin{align*}
        V^{\pi}(s) = \mathbb{E}_\pi \left[r^t + \gamma u^{t+1}| s^{t} = s \right]
    \end{align*}
    \vspace{5pt}
\end{frame}

\begin{frame}[t]{Deriving the Bellman Equation}

    \begin{align*}
        V^{\pi}(\st) &= \ex{\pi}{\rew^t + \gamma u^{t+1} \mid \st^{t} = \st } \\ 
                   &= \sum_{\ac \in \Ac} \pi(\ac \mid \st) \sum_{\st' \in \St} \Stf(\st'\mid \st, \ac) \left[\Rew(\st, \ac, \st') + \gamma \ex{\pi}{u^{t+1} \mid \st^{t+1} = \st'}\right] \\
                   &= \sum_{\ac \in \Ac} \pi(\ac \mid \st) \sum_{\st' \in \St} \Stf(\st'\mid \st, \ac) \left[\Rew(\st, \ac, \st') + \gamma V^{\pi}(\st')\right]
    \end{align*}
    
    The last equation is known as the \e{Bellman equation} in honor of Richard Bellman. 
    
    \blist
        \item The equation states that the value of being in state $s$ while following a fixed policy $\pi$ is equivalent to the immediate reward ($\Rew(\st, \ac, \st') \to \rew$) received when taking action $\ac$ in state $\st$ ($\pi(\ac \mid \st)$), and the subsequent state's \textbf{expected} value. 
    \elist
\end{frame}

\begin{frame}{State-Action Value Function}
    \textit{State-Action} value function $Q^{\pi}(s,a)$ are an extension on the \textit{State} value functions. They condition the expected return on the current state and the action taken.
    \begin{align*}
         Q^{\pi}(s,a) = \mathbb{E}_{\pi}\left[u^{t}|s^{t}=s, a^{t}=a\right]
    \end{align*}

The \textit{state-action} value Bellman equation is therefore:
\vspace{10pt}
\begin{align*}
    Q^{\pi}(s,a) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} \mathcal{T}(s'|s,a) \left[\mathcal{R}(s,a,s') + \gamma Q^{\pi}(s')\right]
\end{align*}

\end{frame}

\begin{frame}{Greedy Policies}

    A policy might act \textit{greedily} i.e. choosing actions which maximize the immediate reward and the value of the next state.
    \vspace{10pt}
    
    Greedy $\pi$ using \textit{state value} functions:
    \vspace{5pt}    
    \begin{align*}
       \pi(s) = \argmax_{a \in A} \sum_{s', r} \mathcal{T}(s', r | s, a) \left[ r + \gamma V(s') \right]
    \end{align*}
    
    Or using the \textit{state-action value function}:
    \vspace{5pt}
    \begin{align*}
        \pi(s) = \argmax_{a \in A}Q(s, a)    
    \end{align*}
    
\end{frame}

\begin{frame}{Optimal Greedy Policy}

   A greedy policy with respect to a value function becomes optimal only when using the \textbf{optimal value function.} 
    
    An \textbf{optimal value function} for a MDP can be defined as:
    \vspace{5pt}
    \begin{align*}
        V^{*}(s) &= \max_{\pi'} V^{\pi'}(s), \quad \forall s \in S \\
        Q^{*}(s, a) &= \max_{\pi'} Q^{\pi'}(s, a), \quad \forall s \in S, a \in A
    \end{align*}

    Therefore, the optimal policy is:
    \vspace{5pt}
    \begin{align*}
        \pi^{*}(s) = \argmax_{a \in A}Q^{*}(s, a)    
    \end{align*}
\end{frame}

\begin{frame}{Dynamic Programming}
    \blist
        \item Dynamic Programming (DP) is a family of algorithms to compute \textbf{optimal value functions} and \textbf{optimal policies in MDPs} (Bellman 1957; Howard 1960).
        \item In DP, we \textbf{assume complete knowledge} of the MDP, including the transition and reward function ($\mathcal{T}, \mathcal{R}$).
        \item Given complete knowledge, we can use the \textbf{Bellman equation} to find optimal value functions and policies.
    \elist   
\end{frame}

\begin{frame}{Policy Iteration}
    \fat{Policy iteration}, is a DP algorithm that alternates between 2 tasks:
        \blist
            \item \textbf{Policy evaluation}: computing value function $V^{\pi}$ for current policy $\pi$.
            \item \textbf{Policy improvement}: improve current policy $\pi$ with respect to $V^{\pi}$.
        \elist 

    \begin{align*}
        \pi^{0} \to V^{\pi^{0}}\to \pi^{1} \to V^{\pi^{1} } \to\pi^{2} \to ... \to V^{*} \to \pi^{*} 
    \end{align*}

    % \begin{figure}
    %     \centering
    %     \includegraphics[width=\textwidth,height=.45\textheight,keepaspectratio]{images/1_policy_iteration.png}
    %     \label{fig:policy_iteration}
    % \end{figure}
\end{frame}

\begin{frame}[fragile]{Policy Iteration Pseudo Code}
\centering
\scalebox{0.8}{\begin{minipage}{\linewidth}
    \balg{Policy Iteration}{pi}
        \State Initialize $\pi$ randomly, initialise $V(s)$ arbitrarily for all $s \in S$ except $V(\text{terminal}) = 0$
        \Repeat
            \State \textbf{Policy Evaluation:}
            \Repeat
                \For{each state $s$ in $S$}
                    \State $V(s) \gets \sum_{s'} \mathcal{T}(s' | s, \pi(s)) [\mathcal{R}(s, \pi(s), s') + \gamma V(s')]$
                \EndFor
            \Until{$V(s)$ converges for all $s \in S$}
            \State \textbf{Policy Improvement:}
            \State policy\_stable $\gets$ true
            \For{each state $s$ in $S$}
                \State old\_action $\gets \pi(s)$
                \State \Cboxed{red}{$\pi(s) \gets \arg\max_a \sum_{s'} \mathcal{T}(s' | s, a) [\mathcal{R}(s, a, s') + \gamma V(s')]$}
                \If{old\_action $\neq \pi(s)$}
                    \State policy\_stable $\gets$ false
                \EndIf
            \EndFor
        \Until{policy\_stable}
        \Return $V, \pi$
    \ealg
\end{minipage}%
    }
\end{frame}


\begin{frame}{Value Iteration}

    \blist
        \item \textbf{Value Iteration} uses the \textbf{Bellman optimality equation}.
        \item This combines iterative policy evaluation and improvement into one single update equation.
    \elist

    \textit{Bellman Optimality Equation as update operator:}
    \begin{align*}
        V^{k+1}(s) \gets \max_{a \in A} \sum_{s' \in S} \mathcal{T}(s' | s, a) [\mathcal{R}(s, a, s') + \gamma V^k(s')], \quad \forall s \in S
    \end{align*}

    \blist
        \item The max operator makes this the Bellman \textit{optimality} equation.
        \item The equation expresses the value of a state as the maximum expected return achievable by taking the best action and then following the optimal policy thereafter.
    \elist
\end{frame}

\begin{frame}[fragile]{Value Iteration Pseudo Code}


    \balg{Value Iteration}{vi}
        \State Initialize: $V(s) = 0, \forall s \in S$
        \Repeat
            \State \Cboxed{red}{$\forall s \in S: V(s) \gets \max_{a \in A} \sum_{s' \in S} \mathcal{T}(s' | s, a) \left[\mathcal{R}(s, a, s') + \gamma V(s')\right]$}
        \Until{$V$ converged}
        \Return{ optimal policy $\pi^{*}$ with:}
        \State $\forall s \in S: \argmax_{a \in A} \sum_{s' \in S}\mathcal{T}(s'|s, a)\left[\mathcal{R}(s, a, s')+\gamma V(s') \right]$
    \ealg

\end{frame}

\begin{frame}{Temporal-Difference Learning}
    \textbf{Temporal-Difference} (TD) learning is a family of RL algorithms which learn optimal policies and value functions based on data collected through environment \textbf{interactions}. 

    \blist
        \item These algorithms learn which actions yield the best returns by 
        \textbf{trial and error} and exploring different actions and states
    \elist
    \vspace{0pt}
    
    Some \textbf{advantages} of this include:
    \vspace{10pt}
    \blist
        \item No need for a \textbf{model} of the environment's \textbf{reward} and \textbf{transition} function ($\mathcal{R}, \mathcal{T}$). 
        \item They can learn \textbf{online}, updating the policy while interacting with the environment. 
    \elist
\end{frame}

\begin{frame}{Temporal Difference Update}
    The update for Temporal Difference learning relies exclusively on value functions.
    \vspace{0pt}
    \begin{align*}
        V(s^{t}) \gets V(s^{t}) + \alpha \left[\mathcal{X} - V(s^{t})\right]
    \end{align*}
    or 
    \vspace{0pt}
    \begin{align*}
        Q(s^{t}, a^{t}) \gets (s^{t}, a^{t})  + \alpha \left[\mathcal{X} - (s^{t}, a^{t})\right]
    \end{align*}

    Where $\mathcal{X}$ is the update target, serving as an estimate of the current state value. $\alpha$ is the learning rate. 
    
\end{frame}

\begin{frame}{Temporal Difference Update}
    
    In SARSA (a basic TD algorithm), we use the experience tuple $(\st^{t}, \ac^{t}, \rew^{t}, \st^{t+1}, \ac^{t+1})$, to construct a target:
    \vspace{10pt}
    \begin{align*}
        \mathcal{X} = \rew^t + \gamma Q(\st^{t+1}, \ac^{t+1})
    \end{align*}
    
    (The immediate reward plus the discounted value of the next state) - note the resemblance of the Bellman equation. 
    \vspace{10pt}
    \begin{align*}
        Q^{\pi}(\st, \ac) = \sum_{\ac \in \Ac} \pi(\ac \mid \st) \sum_{\st' \in \St} \Stf(\st' \mid \st, \ac) \Cboxed{red}{$\left[\Rew(\st, \ac, \st') + \gamma Q^{\pi}(s', a')\right]$}
    \end{align*}
\end{frame}

\begin{frame}{SARSA}
    
The SARSA update rule thus becomes:
\vspace{0pt}
\begin{align*}
  Q(s^t, a^t) \leftarrow Q(s^t, a^t) + \alpha[r^t + \gamma Q(s^{t+1}, a^{t+1})- Q(s^t,a^t)] 
\end{align*}

\blist
    \item Note the TD error $r^t + \gamma Q(s^{t+1}, a^{t+1})- Q(s^t,a^t)$.
    \item The TD update is \textbf{bootstrapped}
    \item Using the value \textbf{estimates} of the next state ($Q(s^{t+1}, a^{t+1})$) to update the current state value ($Q(s^t,a^t)$)
\elist



\end{frame}

\begin{frame}[fragile]{SARSA Pseudo Code}
    \centering 
    \balg{SARSA}{sarsa}
        \State Initialize \( Q(s, a) = 0 \) for all \( s \in S, a \in A \)
        \For{every episode}
            \State Observe initial state \( s^0 \)
            \State With probability \( \epsilon \): choose random action \( a^0 \in A \)
            \State Otherwise: choose action \( a^0 \in \arg\max_a Q(s^0, a) \)
            \For{\( t = 0, 1, 2, \ldots \)}
                \State Apply action \( a^t \), observe reward \( r' \) and next state \( s^{t+1} \)
                \State With probability \( \epsilon \): choose random action \( a^{t+1} \in A \)
                \State Otherwise: choose action \( a^{t+1} \in \arg\max_a Q(s^{t+1}, a) \)
                \State \Cboxed{red}{\( Q(s^t, a^t) \gets Q(s^t, a^t) + \alpha [ r^{t} + \gamma Q(s^{t+1}, a^{t+1}) - Q(s^t, a^t) ] \)}
            \EndFor
        \EndFor
    \ealg
\end{frame}

\begin{frame}{Convergence of SARSA}

    SARSA is guaranteed to converge to the optimal \textbf{state-value function}, for all $S \in S$ and $a \in A$, if:
    \vspace{10pt}
    \blist
        \item All \textit{state-action} pairs are explored infinitely many times. $$\forall s \in S, a \in A: \sum_{k=0}^{\infty} \mathbb{I}(s, a) \to \infty $$
        \item The learning rate $\alpha$ is reduced over time according to the "standard stochastic approximation condition". $$\forall s \in S, a \in A: \sum_{k=0}^{\infty} \alpha_k(s, a) \to \infty \quad \text{and} \quad \sum_{k=0}^{\infty} \alpha_k(s, a)^2 < \infty$$
    \elist        
\end{frame}

\begin{frame}{$\epsilon$-Greedy Policies}

    Using a \textbf{greedy policy} would \textbf{violate the convergence condition} of SARSA (infinite exploration of $S$ and $A$). 
    \vspace{0pt}
    \blist
        \item Intuitively, we must explore a wide range of states and actions to find state action combinations that yield high returns
        \item One solution is to add an \textbf{exploration} parameter $\epsilon \in [0, 1]$. This makes gives us a stochastic \textbf{epsilon-greedy} policy.
    \elist
    \vspace{0pt}
    \begin{align*}
        \pi(a|s) = \begin{cases} 
        1 - \epsilon + \frac{\epsilon}{|A|} & \text{if } a \in \arg\max_{a'} Q(s, a') \\
        \frac{\epsilon}{|A|} & \text{otherwise} 
    \end{cases}
    \end{align*}

    \blist
        \item With probability $1-\epsilon$, the policy chooses the greedy action, and with probability $\epsilon$ chooses an action uniformly at random. 
    \elist
    
\end{frame}

\begin{frame}{Q-Learning}

    Q-learning is a popular TD algorithm which uses the Bellman optimality equation to update its value function estimates.
    \vspace{10pt}
    \blist
        \item By using the Bellman optimality equation, Q-learning directly learns the \textbf{optimal state-action value function}
        \item Q-learning is \textbf{off-policy}, meaning the policy followed to gather experiences is different from the optimized policy
        \item We use the $\epsilon$-greedy policy to collect experiences
    \elist

\end{frame}

\begin{frame}{Q-Learning Update}

    The target in Q-learning uses the \textit{max} operator to target the optimal Q-values directly. 
    \vspace{00pt}
    \begin{align*}
        \mathcal{X} = r^{t} + \gamma \max_{a' \in A}Q(s^{t+1}, a')
    \end{align*}

    The Q-learning update is thus:
    \vspace{0pt}
    \begin{align*}
        Q(s^t, a^t) \gets Q(s^t, a^t) + \alpha \left[ r^t + \gamma \max_{a' \in A} Q(s^{t+1}, a') - Q(s^t, a^t) \right]
    \end{align*}
    
\end{frame}

\begin{frame}{Q-Learning Pseudo Code}
    \centering
    \balg{Q-Learning}{ql}
        \State Initialize $Q(s, a) = 0$  for all $\st \in \St, \ac \in \Ac$
        \For{every episode}
            \For{$t = 0, 1, 2, \ldots$} 
                \State Observe current state $s^t$
                \State With probability $\epsilon$: choose random action $\ac^t \in \Ac$
                \State Otherwise: choose action $\ac^t \in \arg\max_{\ac} Q(\st^t, \ac)$
                \State Apply action $\ac^t$, observe reward $\rew^t$ and next state $\st^{t+1}$
                \State \Cboxed{red}{$Q(\st^t, \ac^t) \gets Q(\st^t, \ac^t) + \alpha \left[ \rew^t + \gamma \max_{\ac'} Q(\st^{t+1}, \ac') - Q(\st^t, \ac^t) \right]$}
            \EndFor
        \EndFor
    \ealg
\end{frame}


\begin{frame}{Evaluating RL Algorithms}

    \begin{columns}
    \begin{column}{0.5\textwidth}
        
        \begin{figure}
            \includegraphics[width=0.45\linewidth, height=0.45\textheight, keepaspectratio]{images/chapter_2/tabular_mdp_returns.pdf}
            \hspace{5pt} % Horizontal space between images
            % Row 1, Image 2
            \includegraphics[width=0.45\linewidth, height=0.45\textheight, keepaspectratio]{images/chapter_2/tabular_mdp_eplength.pdf}
        
            \vspace{5pt} % Vertical space between rows
        
            % Row 2, Image 3
            \includegraphics[width=0.45\linewidth, height=0.45\textheight, keepaspectratio]{images/chapter_2/tabular_mdp_ql_alphas_returns.pdf}
            \hspace{5pt} % Horizontal space between images
            % Row 2, Image 4
            \includegraphics[width=0.45\linewidth, height=0.45\textheight, keepaspectratio]{images/chapter_2/tabular_mdp_ql_eps_returns.pdf}
        \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
            
            \textbf{Y-axis:}

            \blist
                \item {\small Average \textbf{discounted evaluation returns}. This shows us how our greedy policy would perform if we stopped learning at time step T.} 
                \item {\small In some cases, \textbf{undiscounted returns} are easier to interpret and may be used instead. }
            \elist
            
            \textbf{X-axis:}
            \blist 
                \item {\small Cumulative training steps across episodes.} 
                \item {\small Number of episodes can also be used. This might, however, distort the learning speed.} 
            \elist
        \end{column}
        
    \end{columns}

\end{frame}

% \begin{frame}{Evaluating RL Algorithms Continued}
% \small
%   \begin{columns}[T]
%     \begin{column}{0.6\textwidth}
%         \fat{Why Cumulative Time Steps and not n Episodes?}
%         \blist
%             \item Episodes can distort the speed at which an algorithm learns. 
%             \item Maybe, and the algorithm needs a few episodes to converge, but it takes a lot of time to explore in early episodes. 
%         \elist
        
%         \fat{Average Discounted Evaluation Returns.}
%         \blist
%             \item Returns are from a greedy policy with respect to learned action values after T time steps. 
%             \item We average to answer the following question: if we finish learning after T time steps and extract the greedy policy, what expected returns can we expect to achieve with this policy?
%         \elist
%     \end{column}

%     \begin{column}{0.4\textwidth}
%         \fat{What about Undiscounted Returns?}
%         \blist
%             \item We usually use discounted returns as this is the learning objective, but undiscounted rewards can sometimes be easier to interpret. 
%             % \item For example, suppose we want to learn an optimal policy for a video game in which the agent controls a spaceship and receives a +1 score (reward) for each destroyed enemy. If in an episode the agent destroys 10 enemies at various points in time, the undiscounted return will be 10 but the discounted return will be less than 10, making it more difficult to understand how many enemies the agent destroyed.
%         \elist
%     \end{column}

%   \end{columns}

% \end{frame}

\begin{frame}{Summary}

\fat{We covered:}
\blist
    \item Multi-Agent Systems and the case for MARL
    \item MDPs
    \item Value Functions
    \item Dynamic Programming
    \item Temporal Difference Learning (SARSA and Q-Learning)
\elist
\fat{Next we'll cover:}
\blist
    \item Games: Models of Multi-Agent Interaction
\elist

\end{frame}

\begin{frame}{Sources:}

Bellman, Richard. 1957. Dynamic Programming. Princeton University Press.
\vspace{20pt}
Howard, Ronald A. 1960. Dynamic Programming and Markov Processes. John Wiley.
\end{frame}
\end{document}
